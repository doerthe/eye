{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/josd/eye/blob/master/transduction/transduction_dices/observation_prediction_dices.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhe7TwGQgoio"
   },
   "source": [
    "# Transduction from observation to prediction for dices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p24oSjZHgoiy"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "What is [Transduction (machine learning)](https://en.wikipedia.org/wiki/Transduction_(machine_learning%29):\n",
    "\n",
    "> In logic, statistical inference, and supervised learning, transduction or\n",
    "transductive inference is reasoning from observed, specific (training) cases\n",
    "to specific (test) cases. In contrast, induction is reasoning from observed\n",
    "training cases to general rules, which are then applied to the test cases.\n",
    "The distinction is most interesting in cases where the predictions of the\n",
    "transductive model are not achievable by any inductive model. Note that this\n",
    "is caused by transductive inference on different test sets producing mutually\n",
    "inconsistent predictions.\n",
    "\n",
    "What is the Tensor2Tensor [Transformer model](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py):\n",
    "\n",
    "> The Transformer model consists of an encoder and a decoder. Both are stacks\n",
    "of self-attention layers followed by feed-forward layers. This model yields\n",
    "good results on a number of problems, especially in NLP and machine translation.\n",
    "See \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762) for the full\n",
    "description of the model and the results obtained with its early version.\n",
    "\n",
    "![Transformer model](https://pbs.twimg.com/media/DCKhefrUMAE9stK.jpg)\n",
    "\n",
    "> The encoder is composed of a stack of N identical layers. Each layer has\n",
    "two sub-layers. The first is a multi-head self-attention mechanism, and the\n",
    "second is a simple, positionwise fully connected feed-forward network.\n",
    "There is a residual connection around each of the two sub-layers, followed by\n",
    "layer normalization.\n",
    "\n",
    "> The decoder is also composed of a stack of N identical layers. In addition\n",
    "to the two sub-layers in each encoder layer, the decoder inserts a third\n",
    "sub-layer, which performs multi-head attention over the output of the encoder\n",
    "stack. The self-attention sub-layer in the decoder stack is modified to prevent\n",
    "positions from attending to subsequent positions.  This masking, combined with\n",
    "the fact that the output embeddings are offset by one position, ensures that the\n",
    "predictions for position i can depend only on the known outputs at positions\n",
    "less than i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "colab_type": "code",
    "id": "v5WnpUpkg7dO",
    "outputId": "2d53f631-62da-40c5-e817-f00652f467b6"
   },
   "outputs": [],
   "source": [
    "# Preparation\n",
    "\n",
    "# install tensor2tensor\n",
    "! pip install -U tensor2tensor\n",
    "\n",
    "# get the needed resources\n",
    "! curl -o observation_prediction_dices.sh http://josd.github.io/eye/transduction/transduction_dices/observation_prediction_dices.sh\n",
    "! curl -o observation_prediction_dices.py http://josd.github.io/eye/transduction/transduction_dices/observation_prediction_dices.py\n",
    "! curl -o __init__.py http://josd.github.io/eye/transduction/transduction_dices/__init__.py\n",
    "! curl -o sample_dices.observation http://josd.github.io/eye/transduction/transduction_dices/sample_dices.observation\n",
    "! chmod +x observation_prediction_dices.sh\n",
    "\n",
    "# clear data and model\n",
    "% rm -fr /tmp/t2t_data/observation_prediction_dices\n",
    "% rm -fr /tmp/t2t_train/observation_prediction_dices/transformer-transformer_small/\n",
    "\n",
    "# start tensorboard\n",
    "! curl -O https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "! unzip -o ngrok-stable-linux-amd64.zip\n",
    "get_ipython().system_raw('tensorboard --logdir /tmp/t2t_train/observation_prediction_dices/transformer-transformer_small --host 0.0.0.0 --port 6006 &')\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "RYpFkp0uykNE",
    "outputId": "f0de6ebb-03b7-4a1f-972a-8c7e57286f21"
   },
   "outputs": [],
   "source": [
    "# See the observation_prediction_dices problem\n",
    "\n",
    "! pygmentize -g observation_prediction_dices.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "colab_type": "code",
    "id": "5F7TRqbByL1o",
    "outputId": "0690dd05-0143-418f-a669-401984035fbb"
   },
   "outputs": [],
   "source": [
    "# See the observation_prediction_dices script\n",
    "\n",
    "! pygmentize -g observation_prediction_dices.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17039
    },
    "colab_type": "code",
    "id": "XqaxB8hqgoi6",
    "outputId": "a13bd35f-ad18-4146-87db-9a3df1a0f922",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the observation_prediction_dices script\n",
    "\n",
    "! ./observation_prediction_dices.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "ENGaCZllgojm",
    "outputId": "21b34f37-6f51-4aff-fb6f-0d899e128e84",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See the transductions\n",
    "# For each observation the top 3 predictions are shown with their respective log probability\n",
    "\n",
    "! pygmentize -g sample_dices.observation\n",
    "print(\"->-\")\n",
    "! pygmentize -g sample_dices.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.bin import t2t_decoder  # To register the hparams set\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.visualization import attention\n",
    "from tensor2tensor.visualization import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min'\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT THE MODEL YOU WANT TO LOAD HERE!\n",
    "CHECKPOINT = os.path.expanduser('/tmp/t2t_train/observation_prediction_dices/transformer-transformer_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HParams\n",
    "problem_name = 'observation_prediction_dices'\n",
    "data_dir = os.path.expanduser('/tmp/t2t_data/observation_prediction_dices')\n",
    "model_name = \"transformer\"\n",
    "hparams_set = \"transformer_small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import observation_prediction_dices\n",
    "\n",
    "visualizer = visualization.AttentionVisualizer(hparams_set, model_name, data_dir, problem_name, beam_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.Variable(0, dtype=tf.int64, trainable=False, name='global_step')\n",
    "\n",
    "sess = tf.train.MonitoredTrainingSession(\n",
    "    checkpoint_dir=CHECKPOINT,\n",
    "    save_summaries_secs=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"A_THROW\"\n",
    "output_string, inp_text, out_text, att_mats = visualizer.get_vis_data_from_string(sess, input_sentence)\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Visualizations\n",
    "- The layers drop down allow you to view the different Transformer layers, 0-indexed of course.\n",
    "  - Tip: The first layer, last layer and 2nd to last layer are usually the most interpretable.\n",
    "- The attention dropdown allows you to select different pairs of encoder-decoder attentions:\n",
    "  - All: Shows all types of attentions together. NOTE: There is no relation between heads of the same color - between the decoder self attention and decoder-encoder attention since they do not share parameters.\n",
    "  - Input - Input: Shows only the encoder self-attention.\n",
    "  - Input - Output: Shows the decoder’s attention on the encoder. NOTE: Every decoder layer attends to the final layer of encoder so the visualization will show the attention on the final encoder layer regardless of what layer is selected in the drop down.\n",
    "  - Output - Output: Shows only the decoder self-attention. NOTE: The visualization might be slightly misleading in the first layer since the text shown is the target of the decoder, the input to the decoder at layer 0 is this text with a GO symbol prepreded.\n",
    "- The colored squares represent the different attention heads.\n",
    "  - You can hide or show a given head by clicking on it’s color.\n",
    "  - Double clicking a color will hide all other colors, double clicking on a color when it’s the only head showing will show all the heads again.\n",
    "- You can hover over a word to see the individual attention weights for just that position.\n",
    "  - Hovering over the words on the left will show what that position attended to.\n",
    "  - Hovering over the words on the right will show what positions attended to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attention.show(inp_text, out_text, *att_mats)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "observation_prediction_dices.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
