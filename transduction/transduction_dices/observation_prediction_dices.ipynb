{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/josd/eye/blob/master/transduction/transduction_dices/observation_prediction_dices.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhe7TwGQgoio"
   },
   "source": [
    "# Transduction from observation to prediction for dices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "colab_type": "code",
    "id": "v5WnpUpkg7dO",
    "outputId": "28974505-2e7c-4267-db80-8d4ab42d334e"
   },
   "outputs": [],
   "source": [
    "# Preparation\n",
    "\n",
    "% rm -fr ~/t2t_data/observation_prediction_dices\n",
    "% rm -fr ~/t2t_train/observation_prediction_dices/transformer-transformer_small/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "RYpFkp0uykNE",
    "outputId": "91dfd4fe-edaf-495c-9fd2-d0f6d380d7ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensor2tensor.data_generators\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m problem\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensor2tensor.data_generators\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m text_problems\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensor2tensor.utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m registry\r\n",
      "\r\n",
      "\u001b[30;01m@registry.register_problem\u001b[39;49;00m\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mObservationPredictionDices\u001b[39;49;00m(text_problems.Text2TextProblem):\r\n",
      "  \u001b[33m\"\"\"Transduction from observation to prediction for dices.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[30;01m@property\u001b[39;49;00m\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mapprox_vocab_size\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[34m256\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[30;01m@property\u001b[39;49;00m\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mis_generate_per_split\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "    \u001b[37m# generate_data will shard the data into TRAIN and EVAL for us.\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mFalse\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[30;01m@property\u001b[39;49;00m\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mdataset_splits\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Splits of data to produce and number of output shards for each.\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m [{\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msplit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: problem.DatasetSplit.TRAIN,\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mshards\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m7\u001b[39;49;00m,\r\n",
      "    }, {\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msplit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: problem.DatasetSplit.EVAL,\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mshards\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\r\n",
      "    }]\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32mgenerate_samples\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, data_dir, tmp_dir, dataset_split):\r\n",
      "    \u001b[34mdel\u001b[39;49;00m data_dir\r\n",
      "    \u001b[34mdel\u001b[39;49;00m tmp_dir\r\n",
      "    \u001b[34mdel\u001b[39;49;00m dataset_split\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m n \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m10000\u001b[39;49;00m):\r\n",
      "      outcome = random.randint(\u001b[34m1\u001b[39;49;00m, \u001b[34m6\u001b[39;49;00m)\r\n",
      "      \u001b[34myield\u001b[39;49;00m {\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33minputs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mA_THROW \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + \u001b[36mrepr\u001b[39;49;00m(n),\r\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mtargets\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mrepr\u001b[39;49;00m(outcome)\r\n",
      "      }\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "  gen = ObservationPredictionDices.generate_samples(\u001b[36mNone\u001b[39;49;00m, \u001b[36mNone\u001b[39;49;00m, \u001b[36mNone\u001b[39;49;00m, \u001b[36mNone\u001b[39;49;00m)\r\n",
      "  \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m gen:\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(i)\r\n"
     ]
    }
   ],
   "source": [
    "# See the observation_prediction_dices problem\n",
    "\n",
    "! pygmentize -g observation_prediction_dices.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "colab_type": "code",
    "id": "5F7TRqbByL1o",
    "outputId": "bf20eada-d8fa-41b3-f2f0-9d824a038f42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m#!/bin/bash\u001b[39;49;00m\n",
      "\u001b[31mPROBLEM\u001b[39;49;00m=observation_prediction_dices\n",
      "\u001b[31mMODEL\u001b[39;49;00m=transformer\n",
      "\u001b[31mHPARAMS\u001b[39;49;00m=transformer_small\n",
      "\n",
      "\u001b[31mUSER_DIR\u001b[39;49;00m=\u001b[31m$PWD\u001b[39;49;00m\n",
      "\u001b[31mDATA_DIR\u001b[39;49;00m=\u001b[31m$HOME\u001b[39;49;00m/t2t_data/\u001b[31m$PROBLEM\u001b[39;49;00m\n",
      "\u001b[31mTMP_DIR\u001b[39;49;00m=/tmp/t2t_datagen/\u001b[31m$PROBLEM\u001b[39;49;00m\n",
      "\u001b[31mTRAIN_DIR\u001b[39;49;00m=\u001b[31m$HOME\u001b[39;49;00m/t2t_train/\u001b[31m$PROBLEM\u001b[39;49;00m/\u001b[31m$MODEL\u001b[39;49;00m-\u001b[31m$HPARAMS\u001b[39;49;00m\n",
      "\n",
      "mkdir -p \u001b[31m$DATA_DIR\u001b[39;49;00m \u001b[31m$TMP_DIR\u001b[39;49;00m \u001b[31m$TRAIN_DIR\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Generate data\u001b[39;49;00m\n",
      "t2t-datagen \u001b[33m\\\u001b[39;49;00m\n",
      "  --data_dir=\u001b[31m$DATA_DIR\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --problem=\u001b[31m$PROBLEM\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --t2t_usr_dir=\u001b[31m$USER_DIR\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --tmp_dir=\u001b[31m$TMP_DIR\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# Train\u001b[39;49;00m\n",
      "t2t-trainer \u001b[33m\\\u001b[39;49;00m\n",
      "  --data_dir=\u001b[31m$DATA_DIR\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --eval_steps=\u001b[34m10\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --hparams_set=\u001b[31m$HPARAMS\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --local_eval_frequency=\u001b[34m100\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --model=\u001b[31m$MODEL\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --output_dir=\u001b[31m$TRAIN_DIR\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --problem=\u001b[31m$PROBLEM\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --t2t_usr_dir=\u001b[31m$USER_DIR\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --train_steps=\u001b[34m1000\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --worker_gpu_memory_fraction=\u001b[34m0\u001b[39;49;00m.75\n",
      "\n",
      "\u001b[37m# Decode\u001b[39;49;00m\n",
      "t2t-decoder \u001b[33m\\\u001b[39;49;00m\n",
      "  --data_dir=\u001b[31m$DATA_DIR\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --decode_from_file=sample_dices.observation \u001b[33m\\\u001b[39;49;00m\n",
      "  --decode_hparams=\u001b[33m\"beam_size=3,alpha=0.6,return_beams=True,write_beam_scores=True\"\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --decode_to_file=sample_dices.prediction \u001b[33m\\\u001b[39;49;00m\n",
      "  --hparams_set=\u001b[31m$HPARAMS\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --model=\u001b[31m$MODEL\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --output_dir=\u001b[31m$TRAIN_DIR\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --problem=\u001b[31m$PROBLEM\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --t2t_usr_dir=\u001b[31m$USER_DIR\u001b[39;49;00m \u001b[33m\\\u001b[39;49;00m\n",
      "  --worker_gpu_memory_fraction=\u001b[34m0\u001b[39;49;00m.75\n"
     ]
    }
   ],
   "source": [
    "# See the observation_prediction_dices script\n",
    "\n",
    "! pygmentize -g observation_prediction_dices.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 15945
    },
    "colab_type": "code",
    "id": "XqaxB8hqgoi6",
    "outputId": "94b9a540-b1cc-4d0e-f628-33f3f2e0679f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module transduction_dices from path /home/jdroo/github.com/josd/eye/transduction\n",
      "[2018-05-15 00:04:06,412] Importing user module transduction_dices from path /home/jdroo/github.com/josd/eye/transduction\n",
      "INFO:tensorflow:Generating problems:\n",
      "    observation:\n",
      "      * observation_prediction_dices\n",
      "[2018-05-15 00:04:06,418] Generating problems:\n",
      "    observation:\n",
      "      * observation_prediction_dices\n",
      "INFO:tensorflow:Generating data for observation_prediction_dices.\n",
      "[2018-05-15 00:04:06,419] Generating data for observation_prediction_dices.\n",
      "INFO:tensorflow:Generating vocab file: /home/jdroo/t2t_data/observation_prediction_dices/vocab.observation_prediction_dices.256.subwords\n",
      "[2018-05-15 00:04:06,421] Generating vocab file: /home/jdroo/t2t_data/observation_prediction_dices/vocab.observation_prediction_dices.256.subwords\n",
      "INFO:tensorflow:Trying min_count 500\n",
      "[2018-05-15 00:04:06,868] Trying min_count 500\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:06,884] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 47\n",
      "[2018-05-15 00:04:07,544] vocab_size = 47\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:07,545] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 42\n",
      "[2018-05-15 00:04:08,232] vocab_size = 42\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:08,233] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 42\n",
      "[2018-05-15 00:04:08,903] vocab_size = 42\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:08,903] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 42\n",
      "[2018-05-15 00:04:09,588] vocab_size = 42\n",
      "INFO:tensorflow:Trying min_count 250\n",
      "[2018-05-15 00:04:09,591] Trying min_count 250\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:09,604] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 137\n",
      "[2018-05-15 00:04:10,317] vocab_size = 137\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:10,318] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 38\n",
      "[2018-05-15 00:04:10,822] vocab_size = 38\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:10,822] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 132\n",
      "[2018-05-15 00:04:11,520] vocab_size = 132\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:11,520] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 38\n",
      "[2018-05-15 00:04:12,010] vocab_size = 38\n",
      "INFO:tensorflow:Trying min_count 125\n",
      "[2018-05-15 00:04:12,014] Trying min_count 125\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:12,025] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 147\n",
      "[2018-05-15 00:04:12,755] vocab_size = 147\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:12,755] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 128\n",
      "[2018-05-15 00:04:13,275] vocab_size = 128\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:13,275] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 132\n",
      "[2018-05-15 00:04:13,840] vocab_size = 132\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:13,840] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 132\n",
      "[2018-05-15 00:04:14,364] vocab_size = 132\n",
      "INFO:tensorflow:Trying min_count 62\n",
      "[2018-05-15 00:04:14,367] Trying min_count 62\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:14,381] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 247\n",
      "[2018-05-15 00:04:15,054] vocab_size = 247\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:15,054] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:15,476] vocab_size = 232\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:15,476] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:15,931] vocab_size = 232\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:15,931] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:16,428] vocab_size = 232\n",
      "INFO:tensorflow:Trying min_count 31\n",
      "[2018-05-15 00:04:16,430] Trying min_count 31\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:16,443] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 247\n",
      "[2018-05-15 00:04:17,153] vocab_size = 247\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:17,153] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:17,620] vocab_size = 232\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:17,620] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:18,073] vocab_size = 232\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:18,074] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:18,541] vocab_size = 232\n",
      "INFO:tensorflow:Trying min_count 15\n",
      "[2018-05-15 00:04:18,546] Trying min_count 15\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:18,560] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 1057\n",
      "[2018-05-15 00:04:19,375] vocab_size = 1057\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:19,375] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 132\n",
      "[2018-05-15 00:04:19,771] vocab_size = 132\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:19,771] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:20,281] vocab_size = 232\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:20,281] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:20,709] vocab_size = 232\n",
      "INFO:tensorflow:Trying min_count 7\n",
      "[2018-05-15 00:04:20,712] Trying min_count 7\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:20,724] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 2047\n",
      "[2018-05-15 00:04:21,429] vocab_size = 2047\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:21,429] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 942\n",
      "[2018-05-15 00:04:21,813] vocab_size = 942\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:21,814] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 942\n",
      "[2018-05-15 00:04:22,201] vocab_size = 942\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:22,201] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 942\n",
      "[2018-05-15 00:04:22,604] vocab_size = 942\n",
      "INFO:tensorflow:Trying min_count 11\n",
      "[2018-05-15 00:04:22,607] Trying min_count 11\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:22,619] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 1057\n",
      "[2018-05-15 00:04:23,241] vocab_size = 1057\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:23,241] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 942\n",
      "[2018-05-15 00:04:23,634] vocab_size = 942\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:23,634] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 942\n",
      "[2018-05-15 00:04:24,027] vocab_size = 942\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:24,027] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 942\n",
      "[2018-05-15 00:04:24,421] vocab_size = 942\n",
      "INFO:tensorflow:Trying min_count 13\n",
      "[2018-05-15 00:04:24,423] Trying min_count 13\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:24,436] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 1057\n",
      "[2018-05-15 00:04:25,067] vocab_size = 1057\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:25,068] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 132\n",
      "[2018-05-15 00:04:25,445] vocab_size = 132\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:25,445] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:25,927] vocab_size = 232\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:25,927] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:26,421] vocab_size = 232\n",
      "INFO:tensorflow:Trying min_count 12\n",
      "[2018-05-15 00:04:26,424] Trying min_count 12\n",
      "INFO:tensorflow:Iteration 0\n",
      "[2018-05-15 00:04:26,435] Iteration 0\n",
      "INFO:tensorflow:vocab_size = 1057\n",
      "[2018-05-15 00:04:27,169] vocab_size = 1057\n",
      "INFO:tensorflow:Iteration 1\n",
      "[2018-05-15 00:04:27,169] Iteration 1\n",
      "INFO:tensorflow:vocab_size = 132\n",
      "[2018-05-15 00:04:27,626] vocab_size = 132\n",
      "INFO:tensorflow:Iteration 2\n",
      "[2018-05-15 00:04:27,627] Iteration 2\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:28,189] vocab_size = 232\n",
      "INFO:tensorflow:Iteration 3\n",
      "[2018-05-15 00:04:28,190] Iteration 3\n",
      "INFO:tensorflow:vocab_size = 232\n",
      "[2018-05-15 00:04:28,677] vocab_size = 232\n",
      "INFO:tensorflow:Generated 10000 Examples\n",
      "[2018-05-15 00:04:31,751] Generated 10000 Examples\n",
      "INFO:tensorflow:Shuffling data...\n",
      "[2018-05-15 00:04:31,753] Shuffling data...\n",
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module transduction_dices from path /home/jdroo/github.com/josd/eye/transduction\n",
      "[2018-05-15 00:05:02,231] Importing user module transduction_dices from path /home/jdroo/github.com/josd/eye/transduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensor2tensor/utils/trainer_lib.py:151: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "[2018-05-15 00:05:02,517] From /usr/local/lib/python3.5/dist-packages/tensor2tensor/utils/trainer_lib.py:151: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "[2018-05-15 00:05:02,518] schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "[2018-05-15 00:05:02,518] worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "[2018-05-15 00:05:02,518] sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "[2018-05-15 00:05:02,519] Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "[2018-05-15 00:05:02,520] datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "[2018-05-15 00:05:02,521] caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "[2018-05-15 00:05:02,522] ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_is_chief': True, '_num_worker_replicas': 0, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.75\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", '_environment': 'local', '_keep_checkpoint_every_n_hours': 10000, '_save_summary_steps': 100, '_model_dir': '/home/jdroo/t2t_train/observation_prediction_dices/transformer-transformer_small', '_evaluation_master': '', '_tf_random_seed': 1234, 'use_tpu': False, '_task_type': None, '_save_checkpoints_secs': None, '_keep_checkpoint_max': 20, '_save_checkpoints_steps': 100, '_train_distribute': None, 't2t_device_info': {'num_async_replicas': 1}, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f63666936a0>, '_task_id': 0, '_master': '', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f63736c0550>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_ps_replicas': 0, '_log_step_count_steps': 100}\n",
      "[2018-05-15 00:05:02,537] Using config: {'_is_chief': True, '_num_worker_replicas': 0, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.75\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", '_environment': 'local', '_keep_checkpoint_every_n_hours': 10000, '_save_summary_steps': 100, '_model_dir': '/home/jdroo/t2t_train/observation_prediction_dices/transformer-transformer_small', '_evaluation_master': '', '_tf_random_seed': 1234, 'use_tpu': False, '_task_type': None, '_save_checkpoints_secs': None, '_keep_checkpoint_max': 20, '_save_checkpoints_steps': 100, '_train_distribute': None, 't2t_device_info': {'num_async_replicas': 1}, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f63666936a0>, '_task_id': 0, '_master': '', 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f63736c0550>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_ps_replicas': 0, '_log_step_count_steps': 100}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7f63665f9bf8>) includes params argument, but params are not passed to Estimator.\n",
      "[2018-05-15 00:05:02,538] Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7f63665f9bf8>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:ValidationMonitor only works with --schedule=train_and_evaluate\n",
      "[2018-05-15 00:05:02,539] ValidationMonitor only works with --schedule=train_and_evaluate\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensor2tensor/utils/trainer_lib.py:337: Experiment.__init__ (from tensorflow.contrib.learn.python.learn.experiment) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.estimator.train_and_evaluate. You will also have to convert to a tf.estimator.Estimator.\n",
      "[2018-05-15 00:05:02,540] From /usr/local/lib/python3.5/dist-packages/tensor2tensor/utils/trainer_lib.py:337: Experiment.__init__ (from tensorflow.contrib.learn.python.learn.experiment) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.estimator.train_and_evaluate. You will also have to convert to a tf.estimator.Estimator.\n",
      "WARNING:tensorflow:Experiment.continuous_train_and_eval (from tensorflow.contrib.learn.python.learn.experiment) is experimental and may change or be removed at any time, and without warning.\n",
      "[2018-05-15 00:05:02,542] Experiment.continuous_train_and_eval (from tensorflow.contrib.learn.python.learn.experiment) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Training model for 100 steps\n",
      "[2018-05-15 00:05:02,543] Training model for 100 steps\n",
      "INFO:tensorflow:Reading data files from /home/jdroo/t2t_data/observation_prediction_dices/observation_prediction_dices-train*\n",
      "[2018-05-15 00:05:02,600] Reading data files from /home/jdroo/t2t_data/observation_prediction_dices/observation_prediction_dices-train*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 7\n",
      "[2018-05-15 00:05:02,602] partition: 0 num_data_files: 7\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "[2018-05-15 00:05:03,173] Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
      "[2018-05-15 00:05:19,052] Setting T2TModel mode to 'train'\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "[2018-05-15 00:05:19,055] Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_232_256.bottom\n",
      "[2018-05-15 00:05:19,186] Transforming feature 'inputs' with symbol_modality_232_256.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_232_256.targets_bottom\n",
      "[2018-05-15 00:05:20,006] Transforming 'targets' with symbol_modality_232_256.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "[2018-05-15 00:05:20,047] Building model body\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensor2tensor/layers/common_layers.py:555: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "[2018-05-15 00:05:20,412] From /usr/local/lib/python3.5/dist-packages/tensor2tensor/layers/common_layers.py:555: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_232_256.top\n",
      "[2018-05-15 00:05:28,139] Transforming body output with symbol_modality_232_256.top\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensor2tensor/layers/common_layers.py:1814: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "[2018-05-15 00:05:28,365] From /usr/local/lib/python3.5/dist-packages/tensor2tensor/layers/common_layers.py:1814: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Base learning rate: 0.200000\n",
      "[2018-05-15 00:05:28,522] Base learning rate: 0.200000\n",
      "INFO:tensorflow:Trainable Variables Total size: 3748864\n",
      "[2018-05-15 00:05:28,546] Trainable Variables Total size: 3748864\n",
      "INFO:tensorflow:Using optimizer Adam\n",
      "[2018-05-15 00:05:28,547] Using optimizer Adam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "[2018-05-15 00:05:41,618] Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "[2018-05-15 00:05:41,621] Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "[2018-05-15 00:05:48,973] Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "[2018-05-15 00:05:51,334] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "[2018-05-15 00:05:51,500] Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/jdroo/t2t_train/observation_prediction_dices/transformer-transformer_small/model.ckpt.\n",
      "[2018-05-15 00:06:21,379] Saving checkpoints for 1 into /home/jdroo/t2t_train/observation_prediction_dices/transformer-transformer_small/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 5.758975\n",
      "[2018-05-15 00:06:24,017] step = 1, loss = 5.758975\n"
     ]
    }
   ],
   "source": [
    "# Run the observation_prediction_dices script\n",
    "\n",
    "! ./observation_prediction_dices.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "ENGaCZllgojm",
    "outputId": "077664e9-761a-478a-ea26-acf421cd1175"
   },
   "outputs": [],
   "source": [
    "# See the transductions\n",
    "# For each observation the top 3 predictions are shown with their respective log probability\n",
    "\n",
    "! pygmentize -g sample_dices.observation\n",
    "print(\"->-\")\n",
    "! pygmentize -g sample_dices.prediction"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "observation_prediction_dices.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
